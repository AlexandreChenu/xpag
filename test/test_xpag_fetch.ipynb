{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4332318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jax\n",
    "import flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "440f7d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gym-gfetch: \n",
      "|    gym version and path: 0.23.1 ['/Users/chenu/opt/anaconda3/envs/xpag_env/lib/python3.9/site-packages/gym']\n",
      "|    REGISTERING GFetch-v0 from /Users/chenu/Desktop/PhD/github/gym-gfetch/gym_gfetch\n",
      "|    REGISTERING GFetchGoal-v0 from /Users/chenu/Desktop/PhD/github/gym-gfetch/gym_gfetch\n",
      "|    REGISTERING GFetchDCIL-v0 from /Users/chenu/Desktop/PhD/github/gym-gfetch/gym_gfetch\n"
     ]
    }
   ],
   "source": [
    "import xpag\n",
    "from xpag.wrappers import gym_vec_env\n",
    "from xpag.buffers import DefaultEpisodicBuffer\n",
    "from xpag.samplers import DefaultEpisodicSampler, HER\n",
    "from xpag.goalsetters import DefaultGoalSetter\n",
    "from xpag.agents import SAC, SAC_bonus\n",
    "from xpag.tools import learn\n",
    "from xpag.tools import mujoco_notebook_replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8906c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym_gfetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30cbb151",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[1328]: Class GLFWApplicationDelegate is implemented in both /Users/chenu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x14c88a778) and /usr/local/Cellar/glfw/3.3.2/lib/libglfw.3.3.dylib (0x14c929f10). One of the two will be used. Which one is undefined.\n",
      "objc[1328]: Class GLFWWindowDelegate is implemented in both /Users/chenu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x14c88a700) and /usr/local/Cellar/glfw/3.3.2/lib/libglfw.3.3.dylib (0x14c929f38). One of the two will be used. Which one is undefined.\n",
      "objc[1328]: Class GLFWContentView is implemented in both /Users/chenu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x14c88a7a0) and /usr/local/Cellar/glfw/3.3.2/lib/libglfw.3.3.dylib (0x14c929f88). One of the two will be used. Which one is undefined.\n",
      "objc[1328]: Class GLFWWindow is implemented in both /Users/chenu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x14c88a818) and /usr/local/Cellar/glfw/3.3.2/lib/libglfw.3.3.dylib (0x14c92a000). One of the two will be used. Which one is undefined.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name:  obj0\n",
      "filename :\n",
      " /Users/chenu/Desktop/PhD/github/dcil/demos/fetchenv/demo_set/1.demo\n",
      "WARNING: could not import mujoco_py. This means robotics environments will not work\n",
      "self.L_budgets =  [10, 10]\n",
      "name:  obj0\n",
      "filename :\n",
      " /Users/chenu/Desktop/PhD/github/dcil/demos/fetchenv/demo_set/1.demo\n",
      "self.L_budgets =  [10, 10]\n",
      "name:  obj0\n",
      "filename :\n",
      " /Users/chenu/Desktop/PhD/github/dcil/demos/fetchenv/demo_set/1.demo\n",
      "self.L_budgets =  [10, 10]\n",
      "name:  obj0\n",
      "filename :\n",
      " /Users/chenu/Desktop/PhD/github/dcil/demos/fetchenv/demo_set/1.demo\n",
      "self.L_budgets =  [10, 10]\n"
     ]
    }
   ],
   "source": [
    "num_envs = 1  # the number of rollouts in parallel during training\n",
    "env, eval_env, env_info = gym_vec_env('GFetchDCIL-v0', num_envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f83ff5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name:  obj0\n",
      "filename :\n",
      " /Users/chenu/Desktop/PhD/github/dcil/demos/fetchenv/demo_set/1.demo\n",
      "self.L_budgets =  [10, 10]\n",
      "sampler =  <xpag.samplers.HER.HER object at 0x7f7f28670850>\n"
     ]
    }
   ],
   "source": [
    "agent = SAC(\n",
    "    env_info['observation_dim'] if not env_info['is_goalenv']\n",
    "    else env_info['observation_dim'] + env_info['desired_goal_dim'],\n",
    "    env_info['action_dim'],\n",
    "    {}\n",
    ")\n",
    "sampler = DefaultEpisodicSampler() if not env_info['is_goalenv'] else HER(env.env_fns[0]().compute_reward)\n",
    "print(\"sampler = \", sampler)\n",
    "\n",
    "buffer = DefaultEpisodicBuffer(\n",
    "    max_episode_steps=env_info['max_episode_steps'],\n",
    "    buffer_size=1_000_000,\n",
    "    sampler=sampler\n",
    ")\n",
    "goalsetter = DefaultGoalSetter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae169081",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "gd_steps_per_step = 1\n",
    "start_training_after_x_steps = env_info['max_episode_steps'] * 10\n",
    "max_steps = 50_000\n",
    "evaluate_every_x_steps = 5_000\n",
    "save_agent_every_x_steps = 100_000\n",
    "save_dir = os.path.join(os.path.expanduser('~'), 'results', 'xpag', 'train_mujoco')\n",
    "save_episode = True\n",
    "plot_projection = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6351204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from xpag.tools.eval import single_rollout_eval\n",
    "from xpag.tools.utils import hstack\n",
    "from xpag.tools.logging import eval_log_reset\n",
    "from xpag.tools.timing import timing_reset\n",
    "from xpag.buffers import Buffer\n",
    "from xpag.agents.agent import Agent\n",
    "from xpag.goalsetters.goalsetter import GoalSetter\n",
    "from typing import Dict, Any, Union\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af9d8aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_traj(trajs, eval_env, save_dir, it=0):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    \n",
    "    ax.set_xlim((0., 0.8))\n",
    "    ax.set_ylim((0.65, 1.5))\n",
    "    ax.set_zlim((0., 1.8))\n",
    "    \n",
    "    fake_env = eval_env.env_fns[0]().unwrapped\n",
    "    fake_env.reset()\n",
    "    \n",
    "    #print(\"traj = \", traj)\n",
    "    for traj in trajs:\n",
    "        \n",
    "        for i in range(traj[0].shape[0]):\n",
    "            success = False\n",
    "            X = []\n",
    "            Y = []\n",
    "            Z = []\n",
    "            #print(\"\\n desired_goal = \", fake_env.goal)\n",
    "            for goal in traj:\n",
    "                #print(\"achieved_goal = \", goal)\n",
    "                if np.linalg.norm(goal[i] - fake_env.goal) < 0.05:\n",
    "                    #print(\"achieved_goal[i] = \", goal[0])\n",
    "                    success = True\n",
    "                X.append(goal[i][0])\n",
    "                Y.append(goal[i][1])\n",
    "                Z.append(goal[i][2])\n",
    "            if success:\n",
    "                ax.plot(X, Y, Z, c=\"m\")\n",
    "            else:\n",
    "                ax.plot(X, Y, Z, c=\"pink\")\n",
    "\n",
    "    #X_eval = [goal[0] for goal in eval_traj]\n",
    "    #Y_eval = [goal[1] for goal in eval_traj]\n",
    "    #Z_eval = [goal[2] for goal in eval_traj]\n",
    "    #ax.plot(X_eval, Y_eval, Z_eval, c = \"red\")\n",
    "\n",
    "    visu_success_zones(eval_env, ax)\n",
    "    \n",
    "    for azim_ in range(45,360,90):\n",
    "        ax.view_init(elev=0., azim = azim_)\n",
    "        plt.savefig(save_dir + \"/trajs_\" + str(azim_) + \"_it_\" + str(it) + \".png\")\n",
    "    \n",
    "    plt.close(fig)\n",
    "    return\n",
    "\n",
    "def visu_success_zones(eval_env, ax):\n",
    "    \"\"\"\n",
    "    Visualize success zones as sphere of radius eps_success around skill-goals\n",
    "    \"\"\"\n",
    "    fake_env = eval_env.env_fns[0]()\n",
    "    L_states = fake_env.skill_manager.L_states\n",
    "    i = 0\n",
    "    for state in L_states:\n",
    "        goal = fake_env.project_to_goal_space(state)\n",
    "\n",
    "        u, v = np.mgrid[0:2*np.pi:20j, 0:np.pi:10j]\n",
    "\n",
    "        x = goal[0] + 0.075*np.cos(u)*np.sin(v)\n",
    "        y = goal[1] + 0.075*np.sin(u)*np.sin(v)\n",
    "        z = goal[2] + 0.075*np.cos(v)\n",
    "        if i ==0:\n",
    "            ax.plot_wireframe(x, y, z, color=\"red\", alpha = 0.1)\n",
    "        else:\n",
    "            ax.plot_wireframe(x, y, z, color=\"blue\", alpha = 0.1)\n",
    "        i+= 1\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f35d1818",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[           0 steps] [training time (ms) += 0         ] [ep reward: 0.000          ] [success: 0.00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "achieved_goal =  [[0.31544292 1.39724488 0.8232876 ]]\n",
      "Logging in /Users/chenu/results/xpag/train_mujoco\n",
      "name:  obj0\n",
      "filename :\n",
      " /Users/chenu/Desktop/PhD/github/dcil/demos/fetchenv/demo_set/1.demo\n",
      "self.L_budgets =  [10, 10]\n",
      "name:  obj0\n",
      "filename :\n",
      " /Users/chenu/Desktop/PhD/github/dcil/demos/fetchenv/demo_set/1.demo\n",
      "self.L_budgets =  [10, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[        5000 steps] [training time (ms) += 356848    ] [ep reward: 0.000          ] [success: 0.00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name:  obj0\n",
      "filename :\n",
      " /Users/chenu/Desktop/PhD/github/dcil/demos/fetchenv/demo_set/1.demo\n",
      "self.L_budgets =  [10, 10]\n",
      "name:  obj0\n",
      "filename :\n",
      " /Users/chenu/Desktop/PhD/github/dcil/demos/fetchenv/demo_set/1.demo\n",
      "self.L_budgets =  [10, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[       10000 steps] [training time (ms) += 586813    ] [ep reward: 0.000          ] [success: 0.00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name:  obj0\n",
      "filename :\n",
      " /Users/chenu/Desktop/PhD/github/dcil/demos/fetchenv/demo_set/1.demo\n",
      "self.L_budgets =  [10, 10]\n",
      "name:  obj0\n",
      "filename :\n",
      " /Users/chenu/Desktop/PhD/github/dcil/demos/fetchenv/demo_set/1.demo\n",
      "self.L_budgets =  [10, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[       15000 steps] [training time (ms) += 570091    ] [ep reward: 0.000          ] [success: 0.00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name:  obj0\n",
      "filename :\n",
      " /Users/chenu/Desktop/PhD/github/dcil/demos/fetchenv/demo_set/1.demo\n",
      "self.L_budgets =  [10, 10]\n",
      "name:  obj0\n",
      "filename :\n",
      " /Users/chenu/Desktop/PhD/github/dcil/demos/fetchenv/demo_set/1.demo\n",
      "self.L_budgets =  [10, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[       20000 steps] [training time (ms) += 567105    ] [ep reward: 0.000          ] [success: 0.00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name:  obj0\n",
      "filename :\n",
      " /Users/chenu/Desktop/PhD/github/dcil/demos/fetchenv/demo_set/1.demo\n",
      "self.L_budgets =  [10, 10]\n",
      "name:  obj0\n",
      "filename :\n",
      " /Users/chenu/Desktop/PhD/github/dcil/demos/fetchenv/demo_set/1.demo\n",
      "self.L_budgets =  [10, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[       25000 steps] [training time (ms) += 568662    ] [ep reward: 0.000          ] [success: 0.00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name:  obj0\n",
      "filename :\n",
      " /Users/chenu/Desktop/PhD/github/dcil/demos/fetchenv/demo_set/1.demo\n",
      "self.L_budgets =  [10, 10]\n",
      "name:  obj0\n",
      "filename :\n",
      " /Users/chenu/Desktop/PhD/github/dcil/demos/fetchenv/demo_set/1.demo\n",
      "self.L_budgets =  [10, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[       30000 steps] [training time (ms) += 562073    ] [ep reward: 0.000          ] [success: 0.00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name:  obj0\n",
      "filename :\n",
      " /Users/chenu/Desktop/PhD/github/dcil/demos/fetchenv/demo_set/1.demo\n",
      "self.L_budgets =  [10, 10]\n",
      "name:  obj0\n",
      "filename :\n",
      " /Users/chenu/Desktop/PhD/github/dcil/demos/fetchenv/demo_set/1.demo\n",
      "self.L_budgets =  [10, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[       35000 steps] [training time (ms) += 566198    ] [ep reward: 0.000          ] [success: 0.00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name:  obj0\n",
      "filename :\n",
      " /Users/chenu/Desktop/PhD/github/dcil/demos/fetchenv/demo_set/1.demo\n",
      "self.L_budgets =  [10, 10]\n",
      "name:  obj0\n",
      "filename :\n",
      " /Users/chenu/Desktop/PhD/github/dcil/demos/fetchenv/demo_set/1.demo\n",
      "self.L_budgets =  [10, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[       40000 steps] [training time (ms) += 565495    ] [ep reward: 0.000          ] [success: 0.00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name:  obj0\n",
      "filename :\n",
      " /Users/chenu/Desktop/PhD/github/dcil/demos/fetchenv/demo_set/1.demo\n",
      "self.L_budgets =  [10, 10]\n",
      "name:  obj0\n",
      "filename :\n",
      " /Users/chenu/Desktop/PhD/github/dcil/demos/fetchenv/demo_set/1.demo\n",
      "self.L_budgets =  [10, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[       45000 steps] [training time (ms) += 570465    ] [ep reward: 0.000          ] [success: 0.00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name:  obj0\n",
      "filename :\n",
      " /Users/chenu/Desktop/PhD/github/dcil/demos/fetchenv/demo_set/1.demo\n",
      "self.L_budgets =  [10, 10]\n",
      "name:  obj0\n",
      "filename :\n",
      " /Users/chenu/Desktop/PhD/github/dcil/demos/fetchenv/demo_set/1.demo\n",
      "self.L_budgets =  [10, 10]\n"
     ]
    }
   ],
   "source": [
    "eval_log_reset()\n",
    "timing_reset()\n",
    "observation = goalsetter.reset(env.reset())\n",
    "print(\"achieved_goal = \", observation[\"achieved_goal\"])\n",
    "trajs = []\n",
    "traj = []\n",
    "\n",
    "for i in range(max_steps // env_info[\"num_envs\"]):\n",
    "    traj.append(observation[\"achieved_goal\"])\n",
    "    if not i % max(evaluate_every_x_steps // env_info[\"num_envs\"], 1):\n",
    "        single_rollout_eval(\n",
    "            i * env_info[\"num_envs\"],\n",
    "            eval_env,\n",
    "            env_info,\n",
    "            agent,\n",
    "            save_dir=save_dir,\n",
    "            plot_projection=plot_projection,\n",
    "            save_episode=save_episode,\n",
    "        )\n",
    "        plot_traj(trajs, eval_env, save_dir, it=i)\n",
    "        traj = []\n",
    "        trajs = []\n",
    "\n",
    "    if not i % max(save_agent_every_x_steps // env_info[\"num_envs\"], 1):\n",
    "        if save_dir is not None:\n",
    "            agent.save(os.path.join(save_dir, \"agent\"))\n",
    "\n",
    "    if i * env_info[\"num_envs\"] < start_training_after_x_steps:\n",
    "        action = env_info[\"action_space\"].sample()\n",
    "    else:\n",
    "        action = agent.select_action(\n",
    "            observation\n",
    "            if not env_info[\"is_goalenv\"]\n",
    "            else hstack(observation[\"observation\"], observation[\"desired_goal\"]),\n",
    "            deterministic=False,\n",
    "        )\n",
    "        for _ in range(max(round(gd_steps_per_step * env_info[\"num_envs\"]), 1)):\n",
    "            batch = buffer.sample(batch_size)\n",
    "            #print(\"batch reward = \", batch[\"reward\"][:10])\n",
    "            #print(\"\\nbatch.done = \", batch[\"done\"][:5])\n",
    "            #print(\"batch.truncation = \", batch[\"truncation\"][:5])\n",
    "            #print(\"batch.info = \", batch[\"info\"])\n",
    "            info_train = agent.train_on_batch(batch)\n",
    "            #print(\"\\nis_success = \", info_train[\"is_success\"])\n",
    "            #print(\"is_not_relabelled = \", info_train[\"is_not_relabelled\"])\n",
    "            #print(\"next_goal_avail = \", info_train[\"next_goal_avail\"])\n",
    "            #print(\"add_bonus = \", info_train[\"add_bonus\"])\n",
    "            #print(\"rewards = \", max(info_train[\"rewards\"]))\n",
    "            #print(\"\\nmasks = \", info_train[\"masks\"])\n",
    "            #print(\"rewards = \", info_train[\"rewards\"])\n",
    "            #print(\"target_q = \", info_train[\"target_q\"])\n",
    "            #print(\"next_q = \", info_train[\"next_q\"])\n",
    "\n",
    "    next_observation, reward, done, info = goalsetter.step(*env.step(action))\n",
    "    #print(\"reward = \", reward)\n",
    "    #print(\"achieved_goal = \", observation[\"achieved_goal\"])\n",
    "    #print(\"reward.shape = \", reward.shape)\n",
    "    #print(\"action = \", action.shape)\n",
    "    #print(\"done = \", done)\n",
    "    #print(\"info = \", info)\n",
    "    \n",
    "    _info = {}\n",
    "    for key in info[0].keys():\n",
    "        _info[key] = np.array([info[i][key] for i in range(len(info))]).reshape(env_info[\"num_envs\"],-1)\n",
    "    #print(\"_info = \", _info)\n",
    "    \n",
    "    step = {\n",
    "        \"observation\": observation,\n",
    "        \"action\": action.reshape(env_info[\"num_envs\"],-1),\n",
    "        \"reward\": reward.reshape(env_info[\"num_envs\"],-1),\n",
    "        \"truncation\": _info[\"truncation\"],\n",
    "        \"done\": done.reshape(env_info[\"num_envs\"],-1),\n",
    "        \"next_observation\": next_observation,\n",
    "    }\n",
    "    if env_info[\"is_goalenv\"]:\n",
    "        step[\"is_success\"] = _info[\"is_success\"]\n",
    "    buffer.insert(step)\n",
    "    observation = next_observation\n",
    "\n",
    "    if done.max():\n",
    "        traj.append(observation[\"achieved_goal\"])\n",
    "        trajs.append(traj)\n",
    "        traj = []\n",
    "        # use store_done() if the buffer is an episodic buffer\n",
    "        if hasattr(buffer, \"store_done\"):\n",
    "            buffer.store_done()\n",
    "        observation = goalsetter.reset_done(env.reset_done())\n",
    "        #print(\"achieved_goal = \", observation[\"achieved_goal\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e5751a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ddcbb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
